{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f56dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /private/var/folders/8_/zfw109hs3b75p2trmzr4373m0000gn/T/pip-req-build-xnyfyhwc\n",
      "  Running command git clone --filter=blob:none -q https://github.com/huggingface/transformers /private/var/folders/8_/zfw109hs3b75p2trmzr4373m0000gn/T/pip-req-build-xnyfyhwc\n",
      "  Resolved https://github.com/huggingface/transformers to commit 7ec1dc8817a99d16e6f9e0ab94ce4027ef74b72d\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting filelock\n",
      "  Using cached filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.10.31-cp36-cp36m-macosx_10_9_x86_64.whl (294 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/env_pytorch2/lib/python3.6/site-packages (from transformers==4.25.0.dev0) (1.19.2)\n",
      "Collecting requests\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2.tar.gz (359 kB)\n",
      "     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 359 kB 4.2 MB/s            \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp36-cp36m-macosx_10_9_x86_64.whl (189 kB)\n",
      "Collecting importlib-metadata\n",
      "  Using cached importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/env_pytorch2/lib/python3.6/site-packages (from transformers==4.25.0.dev0) (4.62.3)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement huggingface-hub<1.0,>=0.10.0 (from transformers) (from versions: 0.0.1, 0.0.2, 0.0.3rc1, 0.0.3rc2, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.9, 0.0.10, 0.0.11, 0.0.12, 0.0.13, 0.0.14, 0.0.15, 0.0.16, 0.0.17, 0.0.18, 0.0.19, 0.1.0, 0.1.1, 0.1.2, 0.2.0, 0.2.1, 0.4.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for huggingface-hub<1.0,>=0.10.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b49d5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from google.colab import drive\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b034bf4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1.tar.gz (220 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting filelock\n",
      "  Using cached filelock-3.4.1-py3-none-any.whl (9.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: dataclasses in /opt/anaconda3/envs/env_pytorch2/lib/python3.6/site-packages (from transformers) (0.8)\n",
      "Collecting importlib-metadata\n",
      "  Using cached importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp36-cp36m-macosx_10_9_x86_64.whl (189 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/env_pytorch2/lib/python3.6/site-packages (from transformers) (4.62.3)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.10.31-cp36-cp36m-macosx_10_9_x86_64.whl (294 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/env_pytorch2/lib/python3.6/site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/env_pytorch2/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/envs/env_pytorch2/lib/python3.6/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/env_pytorch2/lib/python3.6/site-packages (from requests->transformers) (2020.6.20)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.1.1-py2.py3-none-any.whl (309 kB)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/env_pytorch2/lib/python3.6/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Collecting click\n",
      "  Using cached click-8.0.4-py3-none-any.whl (97 kB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Command errored out with exit status 1:\n",
      "   command: /opt/anaconda3/envs/env_pytorch2/bin/python /opt/anaconda3/envs/env_pytorch2/lib/python3.6/site-packages/pip/_vendor/pep517/in_process/_in_process.py build_wheel /var/folders/8_/zfw109hs3b75p2trmzr4373m0000gn/T/tmpagda03xr\n",
      "       cwd: /private/var/folders/8_/zfw109hs3b75p2trmzr4373m0000gn/T/pip-install-143mecbo/tokenizers_f841d9352fe2400ebb571bea8c3ad6ca\n",
      "  Complete output (51 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.macosx-10.9-x86_64-3.6\n",
      "  creating build/lib.macosx-10.9-x86_64-3.6/tokenizers\n",
      "  copying py_src/tokenizers/__init__.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers\n",
      "  creating build/lib.macosx-10.9-x86_64-3.6/tokenizers/models\n",
      "  copying py_src/tokenizers/models/__init__.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/models\n",
      "  creating build/lib.macosx-10.9-x86_64-3.6/tokenizers/decoders\n",
      "  copying py_src/tokenizers/decoders/__init__.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/decoders\n",
      "  creating build/lib.macosx-10.9-x86_64-3.6/tokenizers/normalizers\n",
      "  copying py_src/tokenizers/normalizers/__init__.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/normalizers\n",
      "  creating build/lib.macosx-10.9-x86_64-3.6/tokenizers/pre_tokenizers\n",
      "  copying py_src/tokenizers/pre_tokenizers/__init__.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/pre_tokenizers\n",
      "  creating build/lib.macosx-10.9-x86_64-3.6/tokenizers/processors\n",
      "  copying py_src/tokenizers/processors/__init__.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/processors\n",
      "  creating build/lib.macosx-10.9-x86_64-3.6/tokenizers/trainers\n",
      "  copying py_src/tokenizers/trainers/__init__.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/trainers\n",
      "  creating build/lib.macosx-10.9-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/byte_level_bpe.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/sentencepiece_unigram.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/sentencepiece_bpe.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/base_tokenizer.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/__init__.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/char_level_bpe.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/implementations\n",
      "  copying py_src/tokenizers/implementations/bert_wordpiece.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/implementations\n",
      "  creating build/lib.macosx-10.9-x86_64-3.6/tokenizers/tools\n",
      "  copying py_src/tokenizers/tools/__init__.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/tools\n",
      "  copying py_src/tokenizers/tools/visualizer.py -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/tools\n",
      "  copying py_src/tokenizers/__init__.pyi -> build/lib.macosx-10.9-x86_64-3.6/tokenizers\n",
      "  copying py_src/tokenizers/models/__init__.pyi -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/models\n",
      "  copying py_src/tokenizers/decoders/__init__.pyi -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/decoders\n",
      "  copying py_src/tokenizers/normalizers/__init__.pyi -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/normalizers\n",
      "  copying py_src/tokenizers/pre_tokenizers/__init__.pyi -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/pre_tokenizers\n",
      "  copying py_src/tokenizers/processors/__init__.pyi -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/processors\n",
      "  copying py_src/tokenizers/trainers/__init__.pyi -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/trainers\n",
      "  copying py_src/tokenizers/tools/visualizer-styles.css -> build/lib.macosx-10.9-x86_64-3.6/tokenizers/tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  ----------------------------------------\u001b[0m\n",
      "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\n",
      "\u001b[?25hFailed to build tokenizers\n",
      "\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b33770e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb082d26",
   "metadata": {},
   "source": [
    "'''\n",
    "Fine-Tuning Steps\n",
    "text, topic encoding\n",
    "1. Create Dataset/Dataloader (train, test, eval)\n",
    "2. Instantiate pre-trained model with desired configuration (frozen gpt-2 with one adapter shared at each later)\n",
    "3. Specify which layers to allow for fine-tuning (opt: include adapter modules)\n",
    "4. Write code for conditional inputs (see GPT_conditional_fine_tuning for motivation) (now just tokenized text)\n",
    "5. Write training loop (expert only)\n",
    "6. Write evaluation loop\n",
    "TODO: check loss function (is it cosine loss)\n",
    "TODO: can gpt2 tokenizer take binary, if not .decode(\"utf-8\")\n",
    "\n",
    "replace text with one of the keywords to get corresponding encoding\n",
    "7. Visualize/check results .generate() \n",
    "\n",
    "data = h5py.file(path) \n",
    "TODO: have compativility with arg parser\n",
    "'''\n",
    "'''\n",
    "dataloader: \n",
    "\n",
    "input text: first sentence (tokenized)\n",
    "topic encoding (SOM output)\n",
    "golden label: rest of paragraph (tokenized)\n",
    "'''\n",
    "'''\n",
    "TODO: talk to you about dataloader \n",
    "unkowns: how to handle attention masks\n",
    "\n",
    "we need seperate text to encoding (without som or baseline text embedder)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23572622",
   "metadata": {},
   "source": [
    "\n",
    "GPT_conditional_fine_tuning = \"https://towardsdatascience.com/conditional-text-generation-by-fine-tuning-gpt-2-11c1a9fc639d\"\n",
    "gpt_fine_tune  = \"https://colab.research.google.com/drive/13dZVYEOMhXhkXWfvSMVM1TTtUDrT6Aeh?usp=sharing#scrollTo=Z474sSC6oe7A\"\n",
    "gpt_fine_tune_2 = \"https://medium.com/swlh/fine-tuning-gpt-2-for-magic-the-gathering-flavour-text-generation-3bafd0f9bb93\"\n",
    "gpt_fine_tune_3 = \"https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb\"\n",
    "adding_layer_end_hugging_face = \"https://towardsdatascience.com/adding-custom-layers-on-top-of-a-hugging-face-model-f1ccdfc257bd\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b58723b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a569ea170bc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# load gpt tokentizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO: we are using different tokenizer that Jordan made? right?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# also have ability to specify some of special tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "# load gpt tokentizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # TODO: we are using different tokenizer that Jordan made? right?\n",
    "# also have ability to specify some of special tokens\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "\n",
    "'''\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "# instantiate the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "'''\n",
    "model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c18f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = list(model.modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e988cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92825edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add1a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b88f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d7b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be1f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, module in enumerate(modules[0:12]):\n",
    "    print(\"ind: {}\".format(ind))\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc3cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in model.modules():\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd5faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: might have to use conv1d which kinda works like linear layer but weights are transposed\n",
    "# aware that we need an another choice would be \n",
    "class AdapterLayer(nn.Module):\n",
    "    def __init__(self, input_dim, downsized_dim, topic_encoding):\n",
    "        super().__init__()\n",
    "        self.topic_encoding = topic_encoding\n",
    "        self.adapter_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, downsized_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(downsized_dim, input_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x_with_topic_encoding = torch.cat(x, self.topic_encoding)\n",
    "        assert len(x_with_topic_encoding) == input_dim\n",
    "        return self.adapter_layer(x_with_topic_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c718bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "GPT2MLP_type = type(model.module_list[13])\n",
    "class GPT_With_Adapter_Modules(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GPT_With_Adapter_Modules, self).__init__()\n",
    "        self.gpt2 = GPT2Model.from_pretrained('gpt2')\n",
    "        self.module_list = nn.ModuleList(self.gpt2.modules())\n",
    "        GPT2MLP_output_size = 768\n",
    "        topic_encoding_size = 414 # TODO Ask Jordan what topic encoding size is\n",
    "        downsize_dim = 64 # TODO: decide number\n",
    "        self.adapter_layer = AdapterLayer(GPT2MLP_output_size+ topic_encoding_size, downsize_dim)\n",
    "    def forward(self, x, topic_encoding):\n",
    "        #99.99% chance this is wrong\n",
    "        for module in self.module_list[1::]:\n",
    "            x = module(x)\n",
    "            if isinstance(module, GPT2MLP_type):\n",
    "                x = torch.cat(x, topic_encoding)\n",
    "        return x\n",
    "\n",
    "model = GPT_With_Adapter_Modules() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06118d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2MLP_type = type(model.module_list[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71036a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5fc996",
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in model.module_list:\n",
    "    if isinstance(module, GPT2MLP_type):\n",
    "        print(module) \n",
    "        print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f523681",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.module_list[13])==z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9273850",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.models.gpt2.modeling_gpt2.GPT2MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb982a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b06ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "# load gpt tokentizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # TODO: we are using different tokenizer that Jordan made? right?\n",
    "# also have ability to specify some of special tokens\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium\n",
    "\n",
    "'''\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "# instantiate the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "'''\n",
    "# model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "class GPT_With_Adapter_Modules(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GPT_With_Adapter_Modules, self).__init__()\n",
    "        self.model = GPT2Model.from_pretrained('gpt2')\n",
    "        self.module_list = nn.ModuleList(self.model)\n",
    "        # adapter layers\n",
    "        input_dim = None\n",
    "        downsized_dim = None\n",
    "        num_adapter_layers = None\n",
    "        self.adapter_layers = [AdapterLayer(input_dim, downsized_dim) * num_adapter_layers]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "#           sequence_output, pooled_output = self.bert(\n",
    "#                ids, \n",
    "#                attention_mask=mask)\n",
    "\n",
    "#           # sequence_output has the following shape: (batch_size, sequence_length, 768)\n",
    "#           linear1_output = self.linear1(sequence_output[:,0,:].view(-1,768)) ## extract the 1st token's embeddings\n",
    "\n",
    "#           linear2_output = self.linear2(linear2_output)\n",
    "\n",
    "\n",
    "model = GPT_With_Adapter_Modules() # You can pass the parameters if required to have more flexible model\n",
    "model.to(torch.device(\"cpu\")) ## can be gpu\n",
    "criterion = nn.CrossEntropyLoss() ## If required define your own criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50871673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> l = [module for module in model.modules() if not isinstance(module, nn.Sequential)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e28e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can do other things like specify hyper parameters, lr scheduer\n",
    "text = \"Replace with topic embedding and text.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881162f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# for i, m in enumerate(model.transformer.h):        \n",
    "#     #Only un-freeze the last n transformer blocks\n",
    "#     if i >= 6:\n",
    "#         for parameter in m.parameters():\n",
    "#             parameter.requires_grad = True \n",
    "\n",
    "# for parameter in model.transformer.ln_f.parameters():        \n",
    "#     parameter.requires_grad = True\n",
    "\n",
    "# for parameter in model.lm_head.parameters():        \n",
    "#     parameter.requires_grad = True\n",
    "\n",
    "# TODO: potentially add adapter modules as alternative to allowing fine-tuning on last n transformer blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf71d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yelp_review_medium\")\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c43bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "# Add a new adapter\n",
    "model.add_adapter(\"topic_encoding\")\n",
    "# Activate the adapter for training\n",
    "model.model.train_adapter(\"topic_encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d83baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_training_colab = \"https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/01_Adapter_Training.ipynb#scrollTo=ju-alwbHmKYA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49067bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U adapter-transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' using adapter modules '''\n",
    "from transformers import RobertaConfig, RobertaModelWithHeads\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=2,\n",
    ")\n",
    "model = RobertaModelWithHeads.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef9695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new adapter\n",
    "model.add_adapter(\"topic_encoding\")\n",
    "# Add a matching classification head\n",
    "model.add_classification_head(\n",
    "    \"rotten_tomatoes\",\n",
    "    num_labels=2,\n",
    "    id2label={ 0: \"üëé\", 1: \"üëç\"}\n",
    "  )\n",
    "# Activate the adapter\n",
    "model.train_adapter(\"rotten_tomatoes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('alex-env2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b7132bfbd47f30a49e4f6a2e3b06bd629bb4288fbff316c07cbc5f27e2ca16dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
